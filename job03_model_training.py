
#ëª¨ë¸í•™ìŠµ(ìŠ¤ì¼€ì¼ ì „)

# import pandas as pd
# from konlpy.tag import Okt
# from gensim.models import Word2Vec
# 
# # ğŸ”¹ 1. CSV ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# file_path = "./data/merged_bible (2).csv"  # íŒŒì¼ ê²½ë¡œ ë³€ê²½ í•„ìš”
# df = pd.read_csv(file_path)
# 
# # ğŸ”¹ 2. ë°ì´í„° ì „ì²˜ë¦¬ (ë§ì”€ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°)
# df.dropna(inplace=True)
# df['content'] = df['content'].astype(str).str.replace(r'[^ê°€-í£\s]', '', regex=True)
# 
# # ğŸ”¹ 3. í˜•íƒœì†Œ ë¶„ì„ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ)
# okt = Okt()
# df['tokenized'] = df['content'].apply(lambda x: [word for word, pos in okt.pos(x, stem=True) if pos in ['Noun', 'Verb', 'Adjective']])
# 
# # ğŸ”¹ 4. Word2Vec ëª¨ë¸ í•™ìŠµ
# sentences = df['tokenized'].tolist()
# word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4, epochs=20)
# 
# # ëª¨ë¸ ì €ì¥ (í•„ìš” ì‹œ)
# word2vec_model.save('./models/word2vec_bible.model')
# print("âœ… Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")



#----------------------------------------------------------------------

#ëª¨ë¸í•™ìŠµ(ìŠ¤ì¼€ì¼ í›„)

import pandas as pd
import random
from gensim.models import Word2Vec

# ğŸ”¹ 1. CSV ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
file_path = "./data/processed_bible.csv"  # ì „ì²˜ë¦¬ëœ ì„±ê²½ ë°ì´í„°
df = pd.read_csv(file_path)

# ğŸ”¹ 2. ì „ì²˜ë¦¬ëœ 'processed' ì»¬ëŸ¼ì„ í™œìš©
df.dropna(subset=['processed'], inplace=True)  # ê²°ì¸¡ê°’ ì œê±°
df['processed'] = df['processed'].astype(str)  # ë¬¸ìì—´ ë³€í™˜

# ğŸ”¹ 3. í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
df['tokenized'] = df['processed'].apply(lambda x: x.split())

# ğŸ”¹ 4. ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
word_freq = {}
for sentence in df['tokenized']:
    for word in sentence:
        word_freq[word] = word_freq.get(word, 0) + 1

# ğŸ”¹ 5. ê³ ë¹ˆë„ ë‹¨ì–´ ì¤„ì´ê¸° (ì˜ˆ: 3000ë²ˆ ì´ìƒ ë‚˜ì˜¤ë©´ 50% í™•ë¥ ë¡œ ì œê±°)
THRESHOLD = 3000  # ì„ê³„ê°’ (ì´ ê°’ ì´ìƒ ë“±ì¥í•˜ë©´ ë¹ˆë„ ì¤„ì´ê¸°)
REDUCTION_PROB = 0.5  # ì œê±° í™•ë¥  (50%)

def reduce_high_freq_words(sentence):
    return [
        word for word in sentence
        if word_freq[word] < THRESHOLD or random.random() > REDUCTION_PROB
    ]

df['filtered'] = df['tokenized'].apply(reduce_high_freq_words)

# ğŸ”¹ 6. Word2Vec ëª¨ë¸ í•™ìŠµ
sentences = df['filtered'].tolist()
word2vec_model = Word2Vec(
    sentences,
    vector_size=100,
    window=5,
    min_count=5,  # ìµœì†Œ ë“±ì¥ íšŸìˆ˜ ì„¤ì •
    workers=4,
    epochs=20,
    sample=1e-4  # ê³ ë¹ˆë„ ë‹¨ì–´ ìë™ ìƒ˜í”Œë§
)

# ğŸ”¹ 7. ëª¨ë¸ ì €ì¥
word2vec_model.save('./models/word2vec_bible_scale.model')
print("âœ… Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! (ê³ ë¹ˆë„ ë‹¨ì–´ ìŠ¤ì¼€ì¼ ê°ì†Œ ì ìš©)")
